---
title: "text analysis"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

Load the libraries, basic setup stuff.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
# knitr::opts_chunk$set(eval = FALSE)

library(devtools)
library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.corpora)
library(quanteda.textstats)
library(tidyverse)
library(utf8)
library(showtext)
library(data.table)
library(stopwords)
library(lubridate)
library(stringi)
library(glue)
library(spacyr)
library(ggplot2)
library(ggthemes)


`%notlike%` <- Negate(`%like%`)


`%notin%` <- Negate(`%in%`)

# spacy_initialize(model = "zh_core_web_lg")

rm(list = ls())

theme_set(theme_classic())

pd.data.path <- "/mnt/c/Users/m/projects/peoples_daily_database/data"
dp.data.path <- "/mnt/c/Users/m/projects/death_penalty_xi_era/data"

##############################################
font_add_google("Noto Sans SC", "chinese")

# MAKE the CORPUS~

# path_data <- system.file("/mnt/c/Users/m/projects/death_penalty_xi_era/data/txt/", package = "readtext")
# my_corpus <- readtext("/mnt/c/Users/m/projects/death_penalty_xi_era/data/txt/")
# df_files_metadata <- read_csv(glue("{pd.data.path}/df_files_metadata.csv"))
# df_files_metadata <- df_files_metadata %>% mutate(fulltext = map_chr(path, ~read_file(.x)))
# df_files_metadata %>% fwrite(glue("{dp.data.path}/pd.dp.metadata.w.fulltext.csv"))
pd.dp.metadata.w.fulltext <- fread(glue("{dp.data.path}/pd.dp.metadata.w.fulltext.csv"))
pd.dp.metadata.w.fulltext[,year := str_extract(date, "^[0-9]{4}")]
pd.corp <- corpus(pd.dp.metadata.w.fulltext, text_field = "fulltext")
ch.stop <- read_lines(glue("{dp.data.path}/stopwords_zh.txt"))

# summary(pd.corp)
############################### ATTEMPT, delete or remove later
```
The list of terms is below; we do some funny stuff where it segments them and then stitches them back together and looks for their freq through the corpus. Here's the result of that. 

```{r}
# pd.corp1 <- corpus_subset(pd.corp, headline %like% "打伞破网 深挖彻查")

terms_of_interest <- list("宽严相济", "少杀慎杀", "严打", "依法治国", "以审判为中心", "从严", "从宽", "司法公正", "冤假错案", "和谐社会",  "中国梦", "邓小平", "胡锦涛", "习近平", "重罪轻判", "判处死刑")

names(terms_of_interest) <- terms_of_interest

## tokenize dictionary values
lis <- map(terms_of_interest, ~stri_c_list(as.list(tokens(.x)), sep = " "))
dict <- dictionary(lis)

## tokenize texts and count
toks <- tokens(pd.corp)
dfm_terms <- dfm(tokens_lookup(toks, dict))

df_freqs <- textstat_frequency(dfm_terms, groups = year)

dt_freqs <- as.data.table(df_freqs)
setnames(dt_freqs, "group", "year")

# Just confirm I got them all. 
`%notin%` <- Negate(`%in%`)
terms_of_interest[terms_of_interest %notin% dt_freqs[,unique(feature)]]

# this is them 
dt_freqs

```
Now we can make some plots based on them. I am going to make a bunch with different settings... depending on how one presents the data, the message could be quite different. Note that this is ALL articles in the pd corpus we have -- ie any that even MENTION death penalty. Therefore the appearance of all the XJP terms is a bit confusing. I think it's because he is probably being mentioned everywhere all the time in PD. 

These are basically the same plot but with a different style of display - dots vs cols. 

```{r}
## Make a plot
dt_freqs %>% 
  ggplot(aes(year, frequency, color = feature)) +
  facet_wrap(~feature) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 10)) +
  geom_point() 


dt_freqs %>% 
  # ggplot(aes(year, frequency, color = feature, label=ifelse(frequency>100,as.character(feature),''))) +
  ggplot(aes(year, frequency)) +
  geom_col() +
  facet_wrap(~feature) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

```
But I think it's interesting to see what happens when letting the scales go free. 


I find the second graph quite compelling for seeing trends in all kinds of things over decades. Kind of surprised by the sparseness of 少杀慎杀 given that it was the watchword for DP reform. But indeed it's only mentioned those few times in PD. I could do a separate saerch for it in the database. 

ALSO! I just realised that these do not include the new sihuan data. I'm going to leave all this as it is just now, then I'm going to read in the sihuan data and redo them with that.....

```{r}

dt_freqs %>% 
  ggplot(aes(year, frequency, color = feature)) +
  geom_point() +
  facet_wrap(~feature, scales = c("free_y")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 5))


dt_freqs %>% 
  ggplot(aes(year, frequency)) +
  geom_col() +
  facet_wrap(~feature, scales = "free_y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 10))


```


Now make same graphs with sihuan data included. From now on it will all include sihuan. The outcome is roughly the same, as expected.

```{r}

# Put all the txt files into a single csv object which is read in as a corpus. This only gets done once. Then it's saved to this project dir.
# my_corpus <- readtext(glue("{pd.data.path}/run_5-sihuan-txt/"))
# df_files_metadata <- read_csv(glue("{pd.data.path}/df_files_metadata_run_5-sihuan.csv"))
# df_files_metadata <- df_files_metadata %>% mutate(fulltext = map_chr(path, ~read_file(.x)))
# df_files_metadata %>% fwrite(glue("{dp.data.path}/pd.dp.metadata.w.fulltext_updated_sihuan.csv"))

pd.dp.metadata.w.fulltext1 <- fread(glue("{dp.data.path}/pd.dp.metadata.w.fulltext.csv"))
pd.dp.metadata.w.fulltext2 <- fread(glue("{dp.data.path}/pd.dp.metadata.w.fulltext.updated.sihuan.csv"))
pd.dp.metadata.w.fulltext <- rbind(pd.dp.metadata.w.fulltext1, pd.dp.metadata.w.fulltext2)

options(datatable.prettyprint.char=20L)

pd.dp.metadata.w.fulltext[,year := str_extract(date, "^[0-9]{4}")]
pd.corp <- corpus(pd.dp.metadata.w.fulltext, text_field = "fulltext")
ch.stop <- read_lines(glue("{dp.data.path}/stopwords_zh.txt"))

## tokenize texts and count
toks <- tokens(pd.corp)
dfm_terms <- dfm(tokens_lookup(toks, dict))

df_freqs <- textstat_frequency(dfm_terms, groups = year)

dt_freqs <- as.data.table(df_freqs)
setnames(dt_freqs, "group", "year")


dt_freqs %>% 
  ggplot(aes(year, frequency, color = feature)) +
  geom_point() +
  facet_wrap(~feature, scales = c("free_y")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 5))


dt_freqs %>% 
  ggplot(aes(year, frequency)) +
  geom_col() +
  facet_wrap(~feature, scales = "free_y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

```

But also curious about those same distributions when ONLY death penalty in title are included....

Ok well there's just not enough text to get anything out of that. the n is only 500 or whatever. 


```{r}
pd.sixing.hed <- pd.dp.metadata.w.fulltext[headline %like% "死刑"]

pd.corp.subset.sixing.hed <- corpus(pd.sixing.hed, text_field = "fulltext")
ch.stop <- read_lines(glue("{dp.data.path}/stopwords_zh.txt"))

## tokenize texts and count
toks <- tokens(pd.corp.subset.sixing.hed)
dfm_terms <- dfm(tokens_lookup(toks, dict))

df_freqs <- textstat_frequency(dfm_terms, groups = year)

dt_freqs <- as.data.table(df_freqs)
setnames(dt_freqs, "group", "year")


dt_freqs %>% 
  ggplot(aes(year, frequency, color = feature)) +
  geom_point() +
  facet_wrap(~feature, scales = c("free_y")) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 5))


dt_freqs %>% 
  ggplot(aes(year, frequency)) +
  geom_col() +
  facet_wrap(~feature, scales = "free_y") +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

```

And here are those other graphs showing just the distribution of 死刑 in the hed.

Also -- wanna see something crazy? The code that is NOT commented out is what I used to make this graph. But look at all the code that I had to write last week or whatever to get the same graph. How bananas is that? Basically just learning while doing this and figuring out how all this works. Painful!

```{r}
pd.sixing.hed %>% 
  count(year) %>% 
  ggplot(aes(year, n)) + 
  theme_classic() +
  # theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title = "'死刑' in headline in People's Daily by year") +
  geom_col() +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

# dt_for_graph1 <- dt_for_graph %>% 
#   mutate(sixing_in_hed = case_when(
#     str_detect(headline, "死刑") ~ 1),
#     sixing_not_hed = case_when(
#     str_detect(headline, "死刑", negate = TRUE) ~ 1),
#     sixing_in_hed = replace_na(sixing_in_hed, 0),
#     sixing_not_hed = replace_na(sixing_not_hed, 0)) %>%  
#   select(headline, year, sixing_in_hed, sixing_not_hed)
# 
# dt_for_graph1 %>% 
#   group_by(year) %>% 
#   summarise(count_sixing_in_hed = n())
# 
# dt_for_graph1 %>% 
#   mutate(year = as.numeric(year)) %>% 
#   group_by(year) %>% 
#   summarise(count_sixing_in_hed = sum(sixing_in_hed),
#             count_sixing_not_hed = sum(sixing_not_hed)) %>% 
#   ggplot(aes(year)) + 
#   geom_line(aes(y = count_sixing_in_hed, colour = "death penalty in headline")) + 
#   geom_line(aes(y = count_sixing_not_hed, colour = "death penalty not in headline"))
# 
# 
# 
# 
# dt_for_graph1 %>% 
#   mutate(year = as.numeric(year)) %>% 
#   group_by(year) %>% 
#   summarise(count_sixing_in_hed = sum(sixing_in_hed),
#             count_sixing_not_hed = sum(sixing_not_hed)) %>% 
#   ggplot(aes(year)) + 
#   geom_line(aes(y = count_sixing_in_hed, colour = "death penalty in headline")) 
# 
# 
# 
# # DP in headline of article
# dt_for_graph %>% 
#   mutate(sixing_in_hed = case_when(
#     str_detect(headline, "死刑") ~ 1),
#     sixing_not_hed = case_when(
#     str_detect(headline, "死刑", negate = TRUE) ~ 1),
#     sixing_in_hed = replace_na(sixing_in_hed, 0),
#     sixing_not_hed = replace_na(sixing_not_hed, 0)) %>%  
#   mutate(date = ymd(date)) %>% 
#   mutate(month = year(date)) %>% 
#   group_by(year) %>% 
#   # summarise(count_sixing_in_hed = sum(sixing_in_hed),
#             # count_sixing_not_hed = sum(sixing_not_hed)) %>%
#   # ungroup() %>%
#   select(year, sixing_in_hed) %>% 
#   filter(sixing_in_hed == 1) %>% 
#   ggplot(aes(x = year)) + 
#   theme_classic() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
#   labs(title = "'死刑' in headline in People's Daily by year") +
#   geom_bar() 
# 
# 
# 
# 
# # DP in body of article
# dt_for_graph %>% 
#   mutate(sixing_in_hed = case_when(
#     str_detect(headline, "死刑") ~ 1),
#     sixing_not_hed = case_when(
#     str_detect(headline, "死刑", negate = TRUE) ~ 1),
#     sixing_in_hed = replace_na(sixing_in_hed, 0),
#     sixing_not_hed = replace_na(sixing_not_hed, 0)) %>%  
#   mutate(date = ymd(date)) %>% 
#   mutate(month = year(date)) %>% 
#   group_by(year) %>% 
#   select(year, sixing_not_hed) %>% 
#   filter(sixing_not_hed == 1) %>% 
#   ggplot(aes(x = year)) + 
#   theme_classic() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
#   labs(title = "'死刑' in body of article in People's Daily by year") +
#   geom_bar() 


# # DP in EITHER body OR headline
# dt_for_graph %>% 
#   mutate(sixing_in_hed = case_when(
#     str_detect(headline, "死刑") ~ 1),
#     sixing_not_hed = case_when(
#     str_detect(headline, "死刑", negate = TRUE) ~ 1),
#     sixing_in_hed = replace_na(sixing_in_hed, 0),
#     sixing_not_hed = replace_na(sixing_not_hed, 0)) %>%  
#   mutate(date = ymd(date)) %>% 
#   mutate(month = year(date)) %>% 
#   group_by(year) %>% 
#   ggplot(aes(x = year)) + 
#   theme_classic() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
#   labs(title = "'死刑' in either the body of article or headline in People's Daily by year") +
#   geom_bar() 
#
```
And this is the overall distribution

```{r}

pd.dp.metadata.w.fulltext.count <- pd.dp.metadata.w.fulltext %>% 
  count(year)

pd.dp.metadata.w.fulltext.count %>% 
  ggplot(aes(year, n)) + 
  theme_classic(base_size = 14) +
  labs(title = "Number of People's Daily articles with 'death penalty' either in headline or body") +
  geom_col(fill = "black") +
  theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "NewCenturySchoolbook")) +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

ggsave("/mnt/c/Users/m/Dropbox/dp_xi_project/plot1.png", dpi = 300)



# theme(panel.background = element_rect(fill='black', colour='black')) +
  # theme(panel.grid.major = element_line(colour = "black") +
  # theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +

################################### Testing Tobias question about 1979 ##############################3

pd.dp.metadata.w.fulltext[year == "1979"][,.(year, path)] %>% View()

pd.dp.metadata.w.fulltext[path %notlike% "/mnt/c"]



```

Now I can recreate the relative frequency analysis data and spin it on 2003-2013 and then subsequently.... Cf https://tutorials.quanteda.io/statistical-analysis/keyness/ for a quick explanation.


```{r}
# subset corpus to only be from 2003 onwards
pd.corp.subset.2003 <- corpus_subset(pd.corp, year >= 2003)
# pd.corp.subset.2003 <- corpus_subset(pd.corp, headline == "打伞破网 深挖彻查")

# This was a time consuming step so I'm saving it out in an Rds and reading it back in.
# toks.spacy.pd.corp.2003 <- spacy_tokenize(pd.corp.subset.2003, remove_punct = TRUE, remove_numbers = TRUE) %>% 
#   as.tokens() %>% 
#   tokens_remove(pattern = ch.stop)
# toks.spacy.pd.corp.2003 %>% write_rds(glue("{dp.data.path}/toks.spacy.pd.corp.2003.Rds"))
toks.spacy.pd.corp.2003  <- read_rds(glue("{dp.data.path}/toks.spacy.pd.corp.2003.Rds"))

# Had to make a bunch of new stop words and take them out of the tokens. 
stop.2003 <- read_lines(glue("{dp.data.path}/stop.2003.txt"))

# printing them so you can see.
print(stop.2003)

# take them out
toks.spacy.pd.corp.2003 <- toks.spacy.pd.corp.2003 %>% 
  tokens_remove(pattern = stop.2003)

# make the document feature matrix
dfm.spacy.pd.corp.subset.2003 <- dfm(toks.spacy.pd.corp.2003)

# give it the same docvars as the original
docvars(dfm.spacy.pd.corp.subset.2003) <- docvars(pd.corp.subset.2003)

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2013)

textplot_keyness(tstat_key)

```

Honestly kinda surprised how uninformative that is??? Very weird. It would be possible to do a bunch of different settings with the corpus subset at different dates, though I'm not sure how much juice we'll get out of that. 

Just for an idea, here's 2005 as the cutoff. 
```{r}

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2005)

textplot_keyness(tstat_key)


```

now 2007 

```{r}

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2007)

textplot_keyness(tstat_key)


```
It could be that such 'keyness' measures are better for other things. 



IGNORE THIS. Haven't figured out how to drive it. It's an unsupervised classifier but I'm not sure this is useful for our purposes. 


```{r}
# textplot_wordcloud(dp_dfm, max_words = 100, font = "SimHei")

dfm.spacy.2003 <- dfm(toks.spacy.pd.corp.2003)
dp_lsa <- textmodel_lsa(dfm.spacy.2003, nd = 10)

sources <- str_remove_all(rownames(dp_lsa$docs), "[0-9///'._txt]") 

sources.color <- rep("gray", times = length(sources))
sources.color[sources %in% "USA"] <- "blue"
sources.color[sources %in% "RUS"] <- "red"

plot(
  dp_lsa$docs[, 4:5],
  col = alpha(sources.color, 0.3),
  pch = 19,
  xlab = "Dimension 4",
  ylab = "Dimension 5",
  main = "LSA dimensions by subcorpus"
)


```


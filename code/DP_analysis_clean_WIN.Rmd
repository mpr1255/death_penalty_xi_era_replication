---
  title: "text analysis"
output:
  html_document:
  df_print: paged
editor_options:
  chunk_output_type: console
---
  
  Load the libraries, basic setup stuff.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
# knitr::opts_chunk$set(eval = FALSE)

library(readtext)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textmodels)
library(quanteda.corpora)
library(quanteda.textstats)
library(tidyverse)
library(utf8)
library(showtext)
library(data.table)
library(stopwords)
library(lubridate)
library(stringi)
library(glue)
library(spacyr)
library(ggplot2)
library(ggthemes)


`%notlike%` <- Negate(`%like%`)


`%notin%` <- Negate(`%in%`)

# spacy_initialize(model = "zh_core_web_lg")

# rm(list = ls())

theme_set(theme_classic())

pd.data.path <- "C://Users//m//projects//peoples_daily_database//data"
dp.data.path <- "C://Users//m//projects//death_penalty_xi_era//data"

# pd.data.path <- "/mnt/c/Users/m/projects/peoples_daily_database/data"
# dp.data.path <- "/mnt/c/Users/m/projects/death_penalty_xi_era/data"

##############################################
font_add_google("Noto Sans SC", "chinese")


########################3


pd.dp.metadata.w.fulltext2[,year := str_extract(date, "^[0-9]{4}")]
pd.dp.metadata.w.fulltext2 %>% count(year)



pd.dp.metadata.w.fulltext1[,year := str_extract(date, "^[0-9]{4}")]
pd.dp.metadata.w.fulltext1 %>% count(year)

pd.dp.metadata.w.fulltext1 %>% count(headline) %>% arrange(desc(n)) %>% print(50)

```


```{r}

pd.dp.metadata.w.fulltext1 <- fread(glue("{dp.data.path}/pd.dp.metadata.w.fulltext.csv"), encoding = "UTF-8")
pd.dp.metadata.w.fulltext2 <- fread(glue("{dp.data.path}/df_files_metadata_run_5-sihuan_WIN.csv"), , encoding = "UTF-8")
pd.dp.metadata.w.fulltext <- rbind(pd.dp.metadata.w.fulltext1, pd.dp.metadata.w.fulltext2, fill = TRUE)
pd.dp.metadata.w.fulltext <- pd.dp.metadata.w.fulltext %>% select(-item)

pd.dp.metadata.w.fulltext[,year := str_extract(date, "^[0-9]{4}")]
pd.corp <- corpus(pd.dp.metadata.w.fulltext, text_field = "fulltext")
ch.stop <- read_lines(glue("{dp.data.path}/stopwords_zh.txt"))

terms_of_interest <- list("宽严相济", "少杀慎杀", "严打", "依法治国", "以审判为中心", "从严", "从宽", "司法公正", "冤假错案", "和谐社会",  "中国梦", "邓小平", "胡锦涛", "习近平", "重罪轻判", "判处死刑")
names(terms_of_interest) <- terms_of_interest


lis <- map(terms_of_interest, ~stri_c_list(as.list(tokens(.x)), sep = " "))
dict <- dictionary(lis)


## tokenize texts and count
toks <- tokens(pd.corp)
dfm_terms <- dfm(tokens_lookup(toks, dict))

# df_freqs <- textstat_frequency(dfm_terms, groups = year)
# 
# dt_freqs <- as.data.table(df_freqs)
# setnames(dt_freqs, "group", "year")


```


```{r}
pd.sixing.hed <- pd.dp.metadata.w.fulltext[headline %like% "死刑"]

pd.sixing.hed %>% 
  count(year) %>% 
  ggplot(aes(year, n)) + 
  theme_classic() +
  labs(title = "'死刑' in headline in People's Daily by year") +
  geom_col() +
  scale_x_discrete(breaks=seq(1940, 2020, 10))

```
And this is the overall distribution

```{r}

pd.dp.metadata.w.fulltext.count <- pd.dp.metadata.w.fulltext %>% 
  count(year)

pd.dp.metadata.w.fulltext.count %>% 
  ggplot(aes(year, n)) + 
  theme_classic(base_size = 14) +
  labs(title = "Number of People's Daily articles with 'death penalty' either in headline or body") +
  geom_col(fill = "black") +
  theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "Decima WE")) +
  scale_x_discrete(breaks=seq(1940, 2020, 10)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot1.png", dpi = 300)

# ggsave("/mnt/c/Users//m//Dropbox//dp_xi_project//plot1.png", dpi = 300)


pd.dp.metadata.w.fulltext.count[year>=2010] %>% 
  ggplot(aes(year, n)) + 
  theme_classic(base_size = 14) +
  labs(title = "Number of People's Daily articles since 2010 with 'death penalty' either in headline or body") +
  geom_col(fill = "black") +
  theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "Decima WE")) +
  scale_x_discrete(breaks=seq(2005, 2020, 5)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot1a_since2010.png", dpi = 300)


filtyear <- 2013
pd.dp.metadata.w.fulltext.count[year>=filtyear] %>% 
  ggplot(aes(year, n)) + 
  theme_classic(base_size = 14) +
  labs(title = glue("Number of People's Daily articles since {filtyear} with 'death penalty' either in headline or body")) +
  geom_col(fill = "blue") +
  theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "Arial")) +
  scale_x_discrete(breaks=seq(filtyear, 2021, 2)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot2_since2013.png", dpi = 300)

############################### Trying lollipop

pd.dp.metadata.w.fulltext.count %>% 
  ggplot(aes(x = year, y = n)) + 
  geom_segment(aes(x=year, xend=year, y=0, yend=n), color="black", size = .5) +
  geom_point(color="orange", size=3) +
  theme_classic(base_size = 14) +
 theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "Decima WE")) +
  labs(title = "Number of People's Daily articles with \n'death penalty' either in headline or body") +
  scale_x_discrete(breaks=seq(1940, 2020, 10)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot1_lollipop.png", width = 7,dpi = 300)


pd.dp.metadata.w.fulltext.count[year>=2010] %>% 
  ggplot(aes(x = year, y = n)) + 
  geom_segment(aes(x=year, xend=year, y=0, yend=n), color="black", size = 1) +
  geom_point(color="orange", size=5) +
  theme_classic() +
  labs(title = "Number of People's Daily articles since 2010 \nwith 'death penalty' either in headline or body") +
  theme(axis.title.x = element_text(color = "black", size = 12, face = "bold"),
  axis.title.y = element_text(color = "black", size = 12, face = "bold"),
  text = element_text(size=14, family= "Decima WE")) +
  scale_x_discrete(breaks=seq(2005, 2020, 5)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot1a_since2010_lollipop.png", width = 7, dpi = 300)


filtyear <- 2013
pd.dp.metadata.w.fulltext.count[year>=filtyear] %>% 
  ggplot(aes(x = year, y = n)) + 
  geom_segment(aes(x=year, xend=year, y=0, yend=n), color="black", size = 1) +
  geom_point(color="orange", size=5) +
  theme_classic(base_size = 14) +
  labs(title = glue("Number of People's Daily articles since {filtyear} \nwith 'death penalty' either in headline or body")) +
  theme(
    panel.grid.major.x = element_blank(),
    panel.border = element_blank(),
    axis.ticks.x = element_blank()
  ) +
  scale_x_discrete(breaks=seq(filtyear, 2021, 2)) 

ggsave("C://Users//m//Dropbox//dp_xi_project//plot2_since2013_lollipop.png", width = 7, dpi = 300)


# theme(panel.background = element_rect(fill='black', colour='black')) +
  # theme(panel.grid.major = element_line(colour = "black") +
  # theme(axis.text.x = element_text(angle = 0, vjust = 0.5, hjust=1)) +

```

Now I can recreate the relative frequency analysis data and spin it on 2003-2013 and then subsequently.... Cf https://tutorials.quanteda.io/statistical-analysis/keyness/ for a quick explanation.


```{r}
# subset corpus to only be from 2003 onwards
pd.corp.subset.2003 <- corpus_subset(pd.corp, year >= 2003)
# pd.corp.subset.2003 <- corpus_subset(pd.corp, headline == "打伞破网 深挖彻查")

# This was a time consuming step so I'm saving it out in an Rds and reading it back in.
# toks.spacy.pd.corp.2003 <- spacy_tokenize(pd.corp.subset.2003, remove_punct = TRUE, remove_numbers = TRUE) %>% 
#   as.tokens() %>% 
#   tokens_remove(pattern = ch.stop)
# toks.spacy.pd.corp.2003 %>% write_rds(glue("{dp.data.path}/toks.spacy.pd.corp.2003.Rds"))
toks.spacy.pd.corp.2003  <- read_rds(glue("{dp.data.path}/toks.spacy.pd.corp.2003.Rds"))

# Had to make a bunch of new stop words and take them out of the tokens. 
stop.2003 <- read_lines(glue("{dp.data.path}/stop.2003.txt"))

# printing them so you can see.
print(stop.2003)

# take them out
toks.spacy.pd.corp.2003 <- toks.spacy.pd.corp.2003 %>% 
  tokens_remove(pattern = stop.2003)

# make the document feature matrix
dfm.spacy.pd.corp.subset.2003 <- dfm(toks.spacy.pd.corp.2003)

# give it the same docvars as the original
docvars(dfm.spacy.pd.corp.subset.2003) <- docvars(pd.corp.subset.2003)

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2013)

textplot_keyness(tstat_key)

```

Honestly kinda surprised how uninformative that is??? Very weird. It would be possible to do a bunch of different settings with the corpus subset at different dates, though I'm not sure how much juice we'll get out of that. 

Just for an idea, here's 2005 as the cutoff. 
```{r}

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2005)

textplot_keyness(tstat_key)


```

now 2007 

```{r}

tstat_key <- textstat_keyness(dfm.spacy.pd.corp.subset.2003, 
                              target = dfm.spacy.pd.corp.subset.2003$year >= 2007)

textplot_keyness(tstat_key)


```
It could be that such 'keyness' measures are better for other things. 



IGNORE THIS. Haven't figured out how to drive it. It's an unsupervised classifier but I'm not sure this is useful for our purposes. 


```{r}
# textplot_wordcloud(dp_dfm, max_words = 100, font = "SimHei")

dfm.spacy.2003 <- dfm(toks.spacy.pd.corp.2003)
dp_lsa <- textmodel_lsa(dfm.spacy.2003, nd = 10)

sources <- str_remove_all(rownames(dp_lsa$docs), "[0-9///'._txt]") 

sources.color <- rep("gray", times = length(sources))
sources.color[sources %in% "USA"] <- "blue"
sources.color[sources %in% "RUS"] <- "red"

plot(
  dp_lsa$docs[, 4:5],
  col = alpha(sources.color, 0.3),
  pch = 19,
  xlab = "Dimension 4",
  ylab = "Dimension 5",
  main = "LSA dimensions by subcorpus"
)


```

